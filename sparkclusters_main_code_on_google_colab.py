# -*- coding: utf-8 -*-
"""SparkClusters_main_code_on_google_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pe8KkklhE0Mhmz3YjGNsvfmSRvssZatO
"""

# This is the Notebook's Link on Google Colab: https://colab.research.google.com/drive/1pe8KkklhE0Mhmz3YjGNsvfmSRvssZatO?usp=sharing

!pip install pyspark
!pip install udocker
!udocker --allow-root install
!udocker --allow-root run <image-repository>:<tag>
!udocker --allow-root ps
!pip install pycountry
!pip install pycountry_convert
!pip install fuzzywuzzy
import os
from google.colab import drive
import pandas as pd
import plotly
import pyspark
import pycountry
import pycountry_convert
import fuzzywuzzy

os.mkdir("/content/src")

os.mkdir("/content/src/input")
os.mkdir("/content/src/output")
os.mkdir("/content/src/jobs")



# to dowload visa_number_in_japan.csv file from the drive

!gdown 1-icdCFKwmmx15S2-vDChsx1JSo5t0bTO -O /content/src/input/visa_number_in_japan.csv

# to dowload spark-clusters_key.pem file from the drive

!gdown 17p3e9-tGZkFpXvsw2XcL64qbC6875hdl -O /content/spark-clusters_key.pem

# to dowload requirements.txt file from the drive

!gdown 1BtCwloHYaY07EJK6QMXhz06cgj45Qe1- -O /content/src/requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd src
!touch docker-compose.yml
!touch Dockerfile.spark
# %cd ..

#Setting docker compose configuration (Services & Networks)
f = open("/content/src/docker-compose.yml", "w")
f.write("""
version: '3'

services:
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    command: bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs
      - ./input:/opt/bitnami/spark/input
      - ./output:/opt/bitnami/spark/output
      - ./requirements.txt:/requirements.txt
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - code-with-yu

  spark-worker: &worker
    build:
      context: .
      dockerfile: Dockerfile.spark
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs
      - ./input:/opt/bitnami/spark/input
      - ./output:/opt/bitnami/spark/output
      - ./requirements.txt:/requirements.txt
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - code-with-yu

  spark-worker-2:
    <<: *worker

  spark-worker-3:
    <<: *worker

  spark-worker-4:
    <<: *worker

networks:
  code-with-yu:
  """)
f.close()



f = open("/content/src/Dockerfile.spark", "w")
f.write("""
FROM bitnami/spark:latest

COPY requirements.txt .

USER root

RUN apt-get clean  && \
	apt-get update && \
	apt-get install -y python3-pip && \
	pip3 install --no-cache-dir -r ./requirements.txt""")
f.close()

# Commented out IPython magic to ensure Python compatibility.
# %cd src/jobs
!touch visualization.py
# %cd ..
# %cd ..

# put this line in cloud cmd to deploy the following job but after uploading it "sudo docker exec -it azureuser-spark-worker-1 spark-submit --master spark://172.18.0.2:7077 jobs/visualization.py"
f = open("/content/src/jobs/visualization.py", "w")
f.write('''
import plotly.express as px
import pycountry
import pycountry_convert as pcc
from fuzzywuzzy import process
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

spark = SparkSession.builder.appName('End to end processing').getOrCreate()

df = spark.read.csv('input/visa_number_in_japan.csv', header=True, inferSchema=True)

# standardize or clean the columns
new_column_names = [col_name.replace(' ', '_')
                    .replace('/', '')
                    .replace('.', '')
                    .replace(',', '')
                    for col_name in df.columns]
# replace the old uncleaned column names with the new standarized ones
df = df.toDF(*new_column_names)

# drop all null columns
df = df.dropna(how='all')

df = df.select('year', 'country', 'number_of_issued_numerical')

def correct_country_name(name, threshold=85):
  countries = [country.name for country in pycountry.countries]

  corrected_name, score = process.extractOne(name, countries)

  if score >= threshold:
    return corrected_name

  # no changes
  return name


def get_continent_name(country_name):
  try:
    country_code = pcc.country_name_to_country_alpha2(country_name, cn_name_format='default')
    continent_code = pcc.country_alpha2_to_continent_code(country_code)
    return pcc.convert_continent_code_to_continent_name(continent_code)
  except:
    return None


correct_country_name_udf = udf(correct_country_name, StringType())
df = df.withColumn('country', correct_country_name_udf(df['country']))

country_corrections = {
  'Andra': 'Russia',
  'Antigua Berbuda': 'Antigua and Barbuda',
  'Barrane': 'Bahrain',
  'Brush': 'Bhutan',
  'Komoro': 'Comoros',
  'Benan': 'Benin',
  'Kiribass': 'Kiribati',
  'Gaiana': 'Guyana',
  'Court Jiboire': "CÃ´te d'Ivoire",
  'Lesot': 'Lesotho',
  'Macau travel certificate': 'Macao',
  'Moldoba': 'Moldova',
  'Naure': 'Nauru',
  'Nigail': 'Niger',
  'Palao': 'Palau',
  'St. Christopher Navis': 'Saint Kitts and Nevis',
  'Santa Principa': 'Sao Tome and Principe',
  'Saechel': 'Seychelles',
  'Slinum': 'Saint Helena',
  'Swaji Land': 'Eswatini',
  'Torque menistan': 'Turkmenistan',
  'Tsubaru': 'Zimbabwe',
  'Kosovo': 'Kosovo'
}

df = df.replace(country_corrections, subset='country')

continent_udf = udf(get_continent_name, StringType())
df = df.withColumn('continent', continent_udf(df['country']))

df.createGlobalTempView('japan_visa')

# VISUALISATION
df_cont = spark.sql("""
    SELECT year, continent, sum(number_of_issued_numerical) visa_issued
    FROM global_temp.japan_visa
    WHERE continent IS NOT NULL
    GROUP BY year, continent
""")

df_cont = df_cont.toPandas()

fig = px.bar(df_cont, x='year', y='visa_issued', color='continent', barmode='group')

fig.update_layout(title_text="Number of visa issued in Japan between 2006 and 2017",
xaxis_title='Year',
yaxis_title='Number of visa issued',
legend_title='Continent')

fig.write_html('output/visa_number_in_japan_continent_2006_2017.html')


# top 10 countries with the most issued visa in 2017
df_country = spark.sql("""
    SELECT country, sum(number_of_issued_numerical) visa_issued
    FROM global_temp.japan_visa
    WHERE country NOT IN ('total', 'others')
    AND country IS NOT NULL
    AND year = 2017
    GROUP BY country
    order by visa_issued DESC
    LIMIT 10
""")

df_country = df_country.toPandas()

fig = px.bar(df_country, x='country', y='visa_issued', color='country')

fig.update_layout(title_text="Top 10 countries with most issued visa in 2017",
                  xaxis_title='Country',
                  yaxis_title='Number of visa issued',
                  legend_title='Country')

fig.write_html('output/visa_number_in_japan_by_country_2017.html')

# display the output on the map
df_country_year_map = spark.sql("""
    SELECT year, country, sum(number_of_issued_numerical) visa_issued
    FROM global_temp.japan_visa
    WHERE country not in ('total', 'others')
    and country is not null
    group by year, country
    ORDER BY year asc
""")

df_country_year_map = df_country_year_map.toPandas()

fig = px.choropleth(df_country_year_map, locations='country',
                    color='visa_issued',
                    hover_name='country',
                    animation_frame='year',
                    range_color=[100000, 100000],
                    color_continuous_scale=px.colors.sequential.Plasma,
                    locationmode='country names',
                    title='Yearly visa issued by countries'
                    )

fig.write_html('output/visa_number_in_japan_year_map.html')

df.write.csv("output/visa_number_in_japan_cleaned.csv", header=True, mode='overwrite')


spark.stop()
''')
f.close()

!chmod 400 spark-clusters_key.pem

!scp -i spark-clusters_key.pem -r ./src/* azureuser@20.173.41.19:/home/azureuser # to copy directories from the local machine to the VM with thier contents of subdirectories and files(upload files from LM to VM)

# In order to connect to the vm using your command line or terminal:
# Navigate to the private key(spark-clusters_key.pem) directory in cmd
# Connect using the following line: ssh -x azureuser@20.173.41.19 -i spark-clusters_key.pem

!scp -i spark-clusters_key.pem -o StrictHostKeyChecking=no -rp azureuser@20.173.41.19:/home/azureuser/output/* /content/src/output # to copy output files in the vm to the output directory on local machine(download files from VM to LM)